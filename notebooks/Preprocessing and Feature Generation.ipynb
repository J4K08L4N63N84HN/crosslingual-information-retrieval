{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing and Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook we import the data, preprocess the data and create features for supervised and unsupervised cross-lingual-information retrieval models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## I. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we import the English and German europarl datasets and combine them into a parallel sentence translation dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.import_data import create_data_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_data_subset(sentence_data_source='../data/external/europarl-v7.de-en.en',\n",
    "#                        sentences_data_target='../data/external/europarl-v7.de-en.de',\n",
    "#                        sample_size=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocessing_class import PreprocessingEuroParl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sentences = PreprocessingEuroParl(df_sampled_path=\"../data/interim/europarl_english_german.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parallel_sentences.dataframe = parallel_sentences.dataframe.iloc[0:1000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_source</th>\n",
       "      <th>text_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>If it is legal, we do not need a debate.</td>\n",
       "      <td>Falls es legal ist, dann brauchen wir keine De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14. Further macro-financial assistance for Geo...</td>\n",
       "      <td>14. Weitere Makrofinanzhilfe für Georgien (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The request came to nothing, firstly because t...</td>\n",
       "      <td>Diese Forderung verlief im Sande, zum einen, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>That is how the judicial system works, and Lor...</td>\n",
       "      <td>So funktioniert der Rechtsstaat, und Lord Beth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Council's refusal to make the Charter of F...</td>\n",
       "      <td>Wir sind daher durch die Weigerung des Rates, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>The results that have been achieved, over many...</td>\n",
       "      <td>Die Ergebnisse, die über viele Jahre hinweg du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>It is our hope that the men and women in Afgha...</td>\n",
       "      <td>Wir wünschen uns, dass auch in Afghanistan die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>Members will therefore receive written answers...</td>\n",
       "      <td>Die Mitglieder werden daher schriftliche Antwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>It will apply not only to products within all ...</td>\n",
       "      <td>Die neue Regelung wird sich nicht nur auf Prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>Obviously, Sharon' s satisfaction refers solel...</td>\n",
       "      <td>Offensichtlich beschränkt sich die Befriedigun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        text_source  \\\n",
       "0      0           If it is legal, we do not need a debate.   \n",
       "1      1  14. Further macro-financial assistance for Geo...   \n",
       "2      2  The request came to nothing, firstly because t...   \n",
       "3      3  That is how the judicial system works, and Lor...   \n",
       "4      4  The Council's refusal to make the Charter of F...   \n",
       "..   ...                                                ...   \n",
       "995  995  The results that have been achieved, over many...   \n",
       "996  996  It is our hope that the men and women in Afgha...   \n",
       "997  997  Members will therefore receive written answers...   \n",
       "998  998  It will apply not only to products within all ...   \n",
       "999  999  Obviously, Sharon' s satisfaction refers solel...   \n",
       "\n",
       "                                           text_target  \n",
       "0    Falls es legal ist, dann brauchen wir keine De...  \n",
       "1          14. Weitere Makrofinanzhilfe für Georgien (  \n",
       "2    Diese Forderung verlief im Sande, zum einen, w...  \n",
       "3    So funktioniert der Rechtsstaat, und Lord Beth...  \n",
       "4    Wir sind daher durch die Weigerung des Rates, ...  \n",
       "..                                                 ...  \n",
       "995  Die Ergebnisse, die über viele Jahre hinweg du...  \n",
       "996  Wir wünschen uns, dass auch in Afghanistan die...  \n",
       "997  Die Mitglieder werden daher schriftliche Antwo...  \n",
       "998  Die neue Regelung wird sich nicht nur auf Prod...  \n",
       "999  Offensichtlich beschränkt sich die Befriedigun...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_sentences.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #duc dataset\n",
    "# parallel_sentences.dataframe['Translation']=1\n",
    "# wrong= PreprocessingEuroParl(sentence_data_source='../data/external/europarl-v7.de-en.en',\n",
    "#                  sentence_data_target='../data/external/europarl-v7.de-en.de',number_datapoints=10000)\n",
    "# import pandas as pd\n",
    "# wrong_data=pd.concat([wrong.dataframe.drop(columns='text_target').reset_index(drop=True),wrong.dataframe['text_target'].sample(frac=1).reset_index(drop=True)],axis=1)\n",
    "# wrong_data['Translation']=0\n",
    "# data=pd.concat([parallel_sentences.dataframe.reset_index(drop=True),wrong_data.reset_index(drop=True)])\n",
    "# import pickle \n",
    "# filehandler = open('../data/processed/dataset_duc.pkl', 'wb') \n",
    "# pickle.dump(data, filehandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we preprocess the parallel sentence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob as textblob_source\n",
    "from textblob_de import TextBlobDE as textblob_target\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stopwords_source = stopwords.words('english')\n",
    "stopwords_target = stopwords.words('german')\n",
    "nlp_source = en_core_web_sm.load()\n",
    "nlp_target = de_core_news_sm.load()\n",
    "embedding_array_source_path = \"../data/interim/proc_5k_src_emb.pkl\"\n",
    "embedding_dictionary_source_path =  \"../data/interim/proc_5k_src_word.pkl\"\n",
    "embedding_array_target_path = \"../data/interim/proc_5k_trg_emb.pkl\"\n",
    "embedding_dictionary_target_path =  \"../data/interim/proc_5k_trg_word.pkl\"\n",
    "number_translations = 1\n",
    "number_pc = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'lemmatize' in 5.354 secs\n",
      "Finished 'tokenize_sentence' in 0.126 secs\n",
      "Finished 'strip_whitespace' in 0.001 secs\n",
      "Finished 'lowercase' in 0.002 secs\n",
      "Finished 'remove_punctuation' in 0.003 secs\n",
      "Finished 'remove_stopwords' in 0.034 secs\n",
      "Finished 'remove_numbers' in 0.005 secs\n",
      "Finished 'create_cleaned_token_embedding' in 5.527 secs\n",
      "Finished 'lemmatize' in 4.951 secs\n",
      "Finished 'tokenize_sentence' in 0.135 secs\n",
      "Finished 'strip_whitespace' in 0.002 secs\n",
      "Finished 'lowercase' in 0.003 secs\n",
      "Finished 'remove_punctuation' in 0.003 secs\n",
      "Finished 'remove_stopwords' in 0.042 secs\n",
      "Finished 'remove_numbers' in 0.006 secs\n",
      "Finished 'create_cleaned_token_embedding' in 5.142 secs\n",
      "Finished 'tokenize_sentence' in 0.124 secs\n",
      "Finished 'strip_whitespace' in 0.001 secs\n",
      "Finished 'lowercase' in 0.002 secs\n",
      "Finished 'create_cleaned_text' in 0.129 secs\n",
      "Finished 'tokenize_sentence' in 0.139 secs\n",
      "Finished 'strip_whitespace' in 0.001 secs\n",
      "Finished 'lowercase' in 0.065 secs\n",
      "Finished 'create_cleaned_text' in 0.206 secs\n",
      "Finished 'number_stopwords' in 0.038 secs\n",
      "Finished 'number_stopwords' in 0.049 secs\n",
      "Finished 'remove_stopwords' in 0.038 secs\n",
      "Finished 'remove_stopwords' in 0.048 secs\n",
      "Finished 'number_punctuations_total' in 0.005 secs\n",
      "Finished 'number_punctuations_total' in 0.005 secs\n",
      "Finished 'number_words' in 0.002 secs\n",
      "Finished 'number_words' in 0.002 secs\n",
      "Finished 'number_unique_words' in 0.012 secs\n",
      "Finished 'number_unique_words' in 0.012 secs\n",
      "Finished 'number_characters' in 0.007 secs\n",
      "Finished 'number_characters' in 0.006 secs\n",
      "Finished 'average_characters' in 0.003 secs\n",
      "Finished 'average_characters' in 0.0 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakob/PycharmProjects/crosslingual-information-retrieval/src/data/preprocess_data.py:292: RuntimeWarning: divide by zero encountered in log\n",
      "  return (character_vector / word_vector).replace(np.nan, 0).replace(np.inf, 0).replace(np.log(0), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_punctuation_marks' in 0.001 secs\n",
      "Finished 'number_pos' in 4.762 secs\n",
      "Finished 'number_pos' in 4.718 secs\n",
      "Finished 'number_pos' in 4.772 secs\n",
      "Finished 'number_pos' in 4.793 secs\n",
      "Finished 'number_pos' in 4.769 secs\n",
      "Finished 'number_pos' in 4.794 secs\n",
      "Finished 'number_times' in 4.877 secs\n",
      "Finished 'number_times' in 4.931 secs\n",
      "Finished 'number_times' in 4.902 secs\n",
      "Finished 'number_times' in 4.985 secs\n",
      "Finished 'number_times' in 5.337 secs\n",
      "Finished 'number_times' in 5.419 secs\n",
      "Finished 'polarity' in 0.212 secs\n",
      "Finished 'polarity' in 3.338 secs\n",
      "Finished 'subjectivity' in 0.145 secs\n",
      "Finished 'subjectivity' in 3.006 secs\n",
      "Finished 'named_numbers' in 0.004 secs\n",
      "Finished 'named_numbers' in 0.004 secs\n",
      "Finished 'load_embeddings' in 0.541 secs\n",
      "Finished 'load_embeddings' in 0.323 secs\n",
      "Finished 'pca_embeddings' in 0.43 secs\n",
      "Finished 'pca_embeddings' in 0.397 secs\n",
      "Finished 'word_embeddings' in 0.719 secs\n",
      "Finished 'word_embeddings' in 0.622 secs\n",
      "Finished 'word_embeddings' in 0.407 secs\n",
      "Finished 'word_embeddings' in 0.424 secs\n",
      "Finished 'create_translation_dictionary' in 2.634 secs\n",
      "Finished 'translate_words' in 0.002 secs\n",
      "Finished 'translate_words' in 0.002 secs\n",
      "Finished 'tf_idf_vector' in 2.18 secs\n",
      "Finished 'tf_idf_vector' in 3.05 secs\n",
      "Finished 'sentence_embedding_average' in 0.116 secs\n",
      "Finished 'sentence_embedding_average' in 0.114 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakob/PycharmProjects/crosslingual-information-retrieval/src/data/preprocess_data.py:570: RuntimeWarning: Mean of empty slice.\n",
      "  return [pd.Series(embedding_dataframe.values.mean(axis=1))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'sentence_embedding_tf_idf' in 1.341 secs\n",
      "Finished 'sentence_embedding_tf_idf' in 1.111 secs\n",
      "Finished 'sentence_embedding_average' in 0.114 secs\n",
      "Finished 'sentence_embedding_average' in 0.111 secs\n",
      "Finished 'sentence_embedding_tf_idf' in 1.246 secs\n",
      "Finished 'sentence_embedding_tf_idf' in 1.149 secs\n",
      "94.1337890625\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "parallel_sentences.preprocess_sentences(stopwords_source, nlp_source, textblob_source,\n",
    "                                               embedding_array_source_path, embedding_dictionary_source_path,\n",
    "                                                stopwords_target,nlp_target, textblob_target,\n",
    "                                               embedding_array_target_path, embedding_dictionary_target_path,\n",
    "                                                number_pc)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token_preprocessed_embedding_source</th>\n",
       "      <th>token_preprocessed_embedding_target</th>\n",
       "      <th>number_stopwords_source</th>\n",
       "      <th>number_stopwords_target</th>\n",
       "      <th>number_punctuations_total_source</th>\n",
       "      <th>number_punctuations_total_target</th>\n",
       "      <th>number_words_source</th>\n",
       "      <th>number_words_target</th>\n",
       "      <th>number_unique_words_source</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_target</th>\n",
       "      <th>sentence_embedding_average_source</th>\n",
       "      <th>sentence_embedding_average_target</th>\n",
       "      <th>sentence_embedding_tf_idf_source</th>\n",
       "      <th>sentence_embedding_tf_idf_target</th>\n",
       "      <th>pca_sentence_embedding_average_source</th>\n",
       "      <th>pca_sentence_embedding_average_target</th>\n",
       "      <th>pca_sentence_embedding_tf_idf_source</th>\n",
       "      <th>pca_sentence_embedding_tf_idf_target</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[legal, need, debate]</td>\n",
       "      <td>[fall, legal, brauchen, debatte, fahren]</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[-0.0547864210481445, -0.001046886279558142, ...</td>\n",
       "      <td>[[-0.0004642516374588013, -0.00480425700079649...</td>\n",
       "      <td>[[-0.03485798347327795, -0.0002601149029797123...</td>\n",
       "      <td>[[-0.0008558102416588422, -0.00275132547459795...</td>\n",
       "      <td>[[-0.18665869534015656, 0.0541328601539135, 0....</td>\n",
       "      <td>[[0.1868635393679142, -0.10517097525298595, 0....</td>\n",
       "      <td>[[-0.1032800996159342, 0.030580838367026447, 0...</td>\n",
       "      <td>[[0.07970107030309546, -0.04937941786176913, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[, macro, financial, assistance, georgia]</td>\n",
       "      <td>[, weitere, makrofinanzhilfe, georgien]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.25233564621573196, ''s': 0.0, ',': 0.0,...</td>\n",
       "      <td>[[-0.07823536987416446, 0.028520435327664018, ...</td>\n",
       "      <td>[[-0.04885647725313902, -0.017179030925035477,...</td>\n",
       "      <td>[[-0.03417419916351019, 0.014746991299733562, ...</td>\n",
       "      <td>[[-0.026505450088098993, -0.010296839819223786...</td>\n",
       "      <td>[[-0.05651350598782301, -0.003383892122656107,...</td>\n",
       "      <td>[[-0.0351869510486722, 0.030359416268765926, -...</td>\n",
       "      <td>[[-0.028320199597242593, -0.003322719201683925...</td>\n",
       "      <td>[[-0.018461613968162153, 0.0168677438071605, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[request, come, nothing, firstly, agreement, c...</td>\n",
       "      <td>[forderung, verlaufen, sand, abkomme, juli, , ...</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.12676620105263722, ''s': 0.0, ',': 0.0,...</td>\n",
       "      <td>[[-0.045134014490759, 0.01751163163135061, -0....</td>\n",
       "      <td>[[-0.05232225576764904, 0.020760945742949843, ...</td>\n",
       "      <td>[[-0.010365411129523827, 0.0038327608980117133...</td>\n",
       "      <td>[[-0.013106617638670265, 0.004686814372174176,...</td>\n",
       "      <td>[[-0.1328783824108541, 0.09318970788735896, 0....</td>\n",
       "      <td>[[0.14227077613274255, 0.0006305481074377894, ...</td>\n",
       "      <td>[[-0.029045820329685612, 0.02108844935256146, ...</td>\n",
       "      <td>[[0.03488908218309481, 0.0008203417196038076, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[judicial, system, work, lord, bethell, certai...</td>\n",
       "      <td>[funktionieren, rechtsstaat, lord, bethell, si...</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[-0.06474494597373101, 0.01477035337414306, -...</td>\n",
       "      <td>[[-0.04374217469659117, -0.018645388384660084,...</td>\n",
       "      <td>[[-0.017277284154384016, 0.005058955207043385,...</td>\n",
       "      <td>[[-0.012080231561605363, -0.005438625653106758...</td>\n",
       "      <td>[[-0.09408186576687373, 0.034677622553247675, ...</td>\n",
       "      <td>[[0.1254079987605413, -0.02755207723627488, 0....</td>\n",
       "      <td>[[-0.023637155804179286, 0.008096865210966515,...</td>\n",
       "      <td>[[0.04454561886093693, -0.013183248965263078, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[council, 's, refusal, make, charter, fundamen...</td>\n",
       "      <td>[daher, weigerung, rat, grundrechtecharta, rec...</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[-0.025971028398579128, 0.013925570925214114,...</td>\n",
       "      <td>[[-0.03412883120795919, 0.008240846121528497, ...</td>\n",
       "      <td>[[-0.0044109827176865655, 0.001966324529453451...</td>\n",
       "      <td>[[-0.007807887274272604, 0.0016770821514765677...</td>\n",
       "      <td>[[-0.14225149118842986, 0.12023432431026147, 0...</td>\n",
       "      <td>[[0.2531268526282575, -0.032746767702822886, 0...</td>\n",
       "      <td>[[-0.027294689589584895, 0.0233857555393358, 0...</td>\n",
       "      <td>[[0.055980738282802846, -0.008415893602827644,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>[result, achieve, many, year, european, form, ...</td>\n",
       "      <td>[ergebnis, jahr, hinweg, europäische, form, so...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[-0.04323588777333498, 0.01629018190890617, -...</td>\n",
       "      <td>[[-0.04303964310222202, 0.02331589128718608, -...</td>\n",
       "      <td>[[-0.009646008540982617, 0.0034379211438927766...</td>\n",
       "      <td>[[-0.00941331191963146, 0.005418964724541407, ...</td>\n",
       "      <td>[[-0.12084609662581768, 0.07701705311912865, 0...</td>\n",
       "      <td>[[0.1528394222776923, -0.004250000515538786, 0...</td>\n",
       "      <td>[[-0.025705371857393317, 0.01730794604275638, ...</td>\n",
       "      <td>[[0.03410446276745788, -0.001262826572331346, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>[hope, man, woman, afghanistan, believe, freed...</td>\n",
       "      <td>[wünschen, afghanistan, mann, frau, freiheit, ...</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[-0.055408882246793884, 0.03557439512683471, ...</td>\n",
       "      <td>[[-0.035435031442081225, 0.028897757121526143,...</td>\n",
       "      <td>[[-0.012152234091674813, 0.007647233675488792,...</td>\n",
       "      <td>[[-0.007682745401818995, 0.0064577388111917615...</td>\n",
       "      <td>[[-0.049277301216007846, 0.09290142847519171, ...</td>\n",
       "      <td>[[0.06168329710250392, -0.01276528722305289, 0...</td>\n",
       "      <td>[[-0.0105165061482805, 0.018708888845933053, 0...</td>\n",
       "      <td>[[0.01446977716742693, -0.003531519313800009, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>[member, therefore, receive, write, answer, qu...</td>\n",
       "      <td>[mitglied, daher, schriftlich, antwort, anfrag...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[-0.0073543175822123885, 0.04231263898933927,...</td>\n",
       "      <td>[[-0.031383881655832134, 0.025362386057774227,...</td>\n",
       "      <td>[[-0.0028732507984971078, 0.016673778483697062...</td>\n",
       "      <td>[[-0.012763970955682755, 0.012213428779897206,...</td>\n",
       "      <td>[[-0.1528842349847158, 0.09079383251567681, 0....</td>\n",
       "      <td>[[0.14628264432152113, 0.009530164301395416, 0...</td>\n",
       "      <td>[[-0.061989143552626434, 0.039891886342773096,...</td>\n",
       "      <td>[[0.058890583197122814, 0.003327974210690777, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>[apply, product, within, eu, member, states, a...</td>\n",
       "      <td>[neue, regelung, produkt, sämtlich, eu-mitglie...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[-0.018103930982761085, -0.015288953320123255...</td>\n",
       "      <td>[[-0.0181253104071532, -0.045000554860702584, ...</td>\n",
       "      <td>[[-0.004551185450077001, -0.007122507993248681...</td>\n",
       "      <td>[[-0.005375928984644692, -0.012707365399049776...</td>\n",
       "      <td>[[-0.1138174117077142, 0.011841099592857063, -...</td>\n",
       "      <td>[[0.1764306053519249, 0.03697663013424192, -0....</td>\n",
       "      <td>[[-0.05131239923001699, 0.008111775647292764, ...</td>\n",
       "      <td>[[0.06409916711630012, 0.005045074107518778, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>[obviously, sharon, satisfaction, refer, solel...</td>\n",
       "      <td>[offensichtlich, beschränken, befriedigung, sh...</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>{'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...</td>\n",
       "      <td>[[0.008127129533224635, 0.0028864297912352616,...</td>\n",
       "      <td>[[-0.059113942811058626, 0.018677427485171292,...</td>\n",
       "      <td>[[0.0018965634156353772, 0.0021116690422347, -...</td>\n",
       "      <td>[[-0.018314888874078682, 0.0059876308799741915...</td>\n",
       "      <td>[[-0.10129138496186998, 0.0690458452122079, 0....</td>\n",
       "      <td>[[0.17408444815211827, -0.025122625494582787, ...</td>\n",
       "      <td>[[-0.03094075530591082, 0.026575178718188583, ...</td>\n",
       "      <td>[[0.056282065514168526, -0.008710949602913673,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                token_preprocessed_embedding_source  \\\n",
       "0      0                              [legal, need, debate]   \n",
       "1      1          [, macro, financial, assistance, georgia]   \n",
       "2      2  [request, come, nothing, firstly, agreement, c...   \n",
       "3      3  [judicial, system, work, lord, bethell, certai...   \n",
       "4      4  [council, 's, refusal, make, charter, fundamen...   \n",
       "..   ...                                                ...   \n",
       "995  995  [result, achieve, many, year, european, form, ...   \n",
       "996  996  [hope, man, woman, afghanistan, believe, freed...   \n",
       "997  997  [member, therefore, receive, write, answer, qu...   \n",
       "998  998  [apply, product, within, eu, member, states, a...   \n",
       "999  999  [obviously, sharon, satisfaction, refer, solel...   \n",
       "\n",
       "                   token_preprocessed_embedding_target  \\\n",
       "0             [fall, legal, brauchen, debatte, fahren]   \n",
       "1              [, weitere, makrofinanzhilfe, georgien]   \n",
       "2    [forderung, verlaufen, sand, abkomme, juli, , ...   \n",
       "3    [funktionieren, rechtsstaat, lord, bethell, si...   \n",
       "4    [daher, weigerung, rat, grundrechtecharta, rec...   \n",
       "..                                                 ...   \n",
       "995  [ergebnis, jahr, hinweg, europäische, form, so...   \n",
       "996  [wünschen, afghanistan, mann, frau, freiheit, ...   \n",
       "997  [mitglied, daher, schriftlich, antwort, anfrag...   \n",
       "998  [neue, regelung, produkt, sämtlich, eu-mitglie...   \n",
       "999  [offensichtlich, beschränken, befriedigung, sh...   \n",
       "\n",
       "     number_stopwords_source  number_stopwords_target  \\\n",
       "0                          7                        6   \n",
       "1                          2                        1   \n",
       "2                         23                       22   \n",
       "3                         17                       12   \n",
       "4                         24                       27   \n",
       "..                       ...                      ...   \n",
       "995                       19                       20   \n",
       "996                       18                       20   \n",
       "997                        3                        4   \n",
       "998                        9                       10   \n",
       "999                        6                       10   \n",
       "\n",
       "     number_punctuations_total_source  number_punctuations_total_target  \\\n",
       "0                                   1                                 1   \n",
       "1                                   1                                 1   \n",
       "2                                   3                                 5   \n",
       "3                                   1                                 3   \n",
       "4                                   6                                 5   \n",
       "..                                ...                               ...   \n",
       "995                                 5                                 5   \n",
       "996                                 0                                 4   \n",
       "997                                 0                                 0   \n",
       "998                                 1                                 1   \n",
       "999                                 3                                 0   \n",
       "\n",
       "     number_words_source  number_words_target  number_unique_words_source  \\\n",
       "0                      3                    5                           3   \n",
       "1                      4                    4                           4   \n",
       "2                     22                   16                          22   \n",
       "3                     14                   12                          13   \n",
       "4                     31                   21                          27   \n",
       "..                   ...                  ...                         ...   \n",
       "995                   24                   22                          22   \n",
       "996                   20                   19                          20   \n",
       "997                    6                    6                           6   \n",
       "998                   10                    9                           8   \n",
       "999                    9                   10                           9   \n",
       "\n",
       "     ...                                      tf_idf_target  \\\n",
       "0    ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "1    ...  {'': 0.25233564621573196, ''s': 0.0, ',': 0.0,...   \n",
       "2    ...  {'': 0.12676620105263722, ''s': 0.0, ',': 0.0,...   \n",
       "3    ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "4    ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "..   ...                                                ...   \n",
       "995  ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "996  ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "997  ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "998  ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "999  ...  {'': 0.0, ''s': 0.0, ',': 0.0, '--': 0.0, '-b'...   \n",
       "\n",
       "                     sentence_embedding_average_source  \\\n",
       "0    [[-0.0547864210481445, -0.001046886279558142, ...   \n",
       "1    [[-0.07823536987416446, 0.028520435327664018, ...   \n",
       "2    [[-0.045134014490759, 0.01751163163135061, -0....   \n",
       "3    [[-0.06474494597373101, 0.01477035337414306, -...   \n",
       "4    [[-0.025971028398579128, 0.013925570925214114,...   \n",
       "..                                                 ...   \n",
       "995  [[-0.04323588777333498, 0.01629018190890617, -...   \n",
       "996  [[-0.055408882246793884, 0.03557439512683471, ...   \n",
       "997  [[-0.0073543175822123885, 0.04231263898933927,...   \n",
       "998  [[-0.018103930982761085, -0.015288953320123255...   \n",
       "999  [[0.008127129533224635, 0.0028864297912352616,...   \n",
       "\n",
       "                     sentence_embedding_average_target  \\\n",
       "0    [[-0.0004642516374588013, -0.00480425700079649...   \n",
       "1    [[-0.04885647725313902, -0.017179030925035477,...   \n",
       "2    [[-0.05232225576764904, 0.020760945742949843, ...   \n",
       "3    [[-0.04374217469659117, -0.018645388384660084,...   \n",
       "4    [[-0.03412883120795919, 0.008240846121528497, ...   \n",
       "..                                                 ...   \n",
       "995  [[-0.04303964310222202, 0.02331589128718608, -...   \n",
       "996  [[-0.035435031442081225, 0.028897757121526143,...   \n",
       "997  [[-0.031383881655832134, 0.025362386057774227,...   \n",
       "998  [[-0.0181253104071532, -0.045000554860702584, ...   \n",
       "999  [[-0.059113942811058626, 0.018677427485171292,...   \n",
       "\n",
       "                      sentence_embedding_tf_idf_source  \\\n",
       "0    [[-0.03485798347327795, -0.0002601149029797123...   \n",
       "1    [[-0.03417419916351019, 0.014746991299733562, ...   \n",
       "2    [[-0.010365411129523827, 0.0038327608980117133...   \n",
       "3    [[-0.017277284154384016, 0.005058955207043385,...   \n",
       "4    [[-0.0044109827176865655, 0.001966324529453451...   \n",
       "..                                                 ...   \n",
       "995  [[-0.009646008540982617, 0.0034379211438927766...   \n",
       "996  [[-0.012152234091674813, 0.007647233675488792,...   \n",
       "997  [[-0.0028732507984971078, 0.016673778483697062...   \n",
       "998  [[-0.004551185450077001, -0.007122507993248681...   \n",
       "999  [[0.0018965634156353772, 0.0021116690422347, -...   \n",
       "\n",
       "                      sentence_embedding_tf_idf_target  \\\n",
       "0    [[-0.0008558102416588422, -0.00275132547459795...   \n",
       "1    [[-0.026505450088098993, -0.010296839819223786...   \n",
       "2    [[-0.013106617638670265, 0.004686814372174176,...   \n",
       "3    [[-0.012080231561605363, -0.005438625653106758...   \n",
       "4    [[-0.007807887274272604, 0.0016770821514765677...   \n",
       "..                                                 ...   \n",
       "995  [[-0.00941331191963146, 0.005418964724541407, ...   \n",
       "996  [[-0.007682745401818995, 0.0064577388111917615...   \n",
       "997  [[-0.012763970955682755, 0.012213428779897206,...   \n",
       "998  [[-0.005375928984644692, -0.012707365399049776...   \n",
       "999  [[-0.018314888874078682, 0.0059876308799741915...   \n",
       "\n",
       "                 pca_sentence_embedding_average_source  \\\n",
       "0    [[-0.18665869534015656, 0.0541328601539135, 0....   \n",
       "1    [[-0.05651350598782301, -0.003383892122656107,...   \n",
       "2    [[-0.1328783824108541, 0.09318970788735896, 0....   \n",
       "3    [[-0.09408186576687373, 0.034677622553247675, ...   \n",
       "4    [[-0.14225149118842986, 0.12023432431026147, 0...   \n",
       "..                                                 ...   \n",
       "995  [[-0.12084609662581768, 0.07701705311912865, 0...   \n",
       "996  [[-0.049277301216007846, 0.09290142847519171, ...   \n",
       "997  [[-0.1528842349847158, 0.09079383251567681, 0....   \n",
       "998  [[-0.1138174117077142, 0.011841099592857063, -...   \n",
       "999  [[-0.10129138496186998, 0.0690458452122079, 0....   \n",
       "\n",
       "                 pca_sentence_embedding_average_target  \\\n",
       "0    [[0.1868635393679142, -0.10517097525298595, 0....   \n",
       "1    [[-0.0351869510486722, 0.030359416268765926, -...   \n",
       "2    [[0.14227077613274255, 0.0006305481074377894, ...   \n",
       "3    [[0.1254079987605413, -0.02755207723627488, 0....   \n",
       "4    [[0.2531268526282575, -0.032746767702822886, 0...   \n",
       "..                                                 ...   \n",
       "995  [[0.1528394222776923, -0.004250000515538786, 0...   \n",
       "996  [[0.06168329710250392, -0.01276528722305289, 0...   \n",
       "997  [[0.14628264432152113, 0.009530164301395416, 0...   \n",
       "998  [[0.1764306053519249, 0.03697663013424192, -0....   \n",
       "999  [[0.17408444815211827, -0.025122625494582787, ...   \n",
       "\n",
       "                  pca_sentence_embedding_tf_idf_source  \\\n",
       "0    [[-0.1032800996159342, 0.030580838367026447, 0...   \n",
       "1    [[-0.028320199597242593, -0.003322719201683925...   \n",
       "2    [[-0.029045820329685612, 0.02108844935256146, ...   \n",
       "3    [[-0.023637155804179286, 0.008096865210966515,...   \n",
       "4    [[-0.027294689589584895, 0.0233857555393358, 0...   \n",
       "..                                                 ...   \n",
       "995  [[-0.025705371857393317, 0.01730794604275638, ...   \n",
       "996  [[-0.0105165061482805, 0.018708888845933053, 0...   \n",
       "997  [[-0.061989143552626434, 0.039891886342773096,...   \n",
       "998  [[-0.05131239923001699, 0.008111775647292764, ...   \n",
       "999  [[-0.03094075530591082, 0.026575178718188583, ...   \n",
       "\n",
       "                  pca_sentence_embedding_tf_idf_target  Translation  \n",
       "0    [[0.07970107030309546, -0.04937941786176913, 0...            1  \n",
       "1    [[-0.018461613968162153, 0.0168677438071605, -...            1  \n",
       "2    [[0.03488908218309481, 0.0008203417196038076, ...            1  \n",
       "3    [[0.04454561886093693, -0.013183248965263078, ...            1  \n",
       "4    [[0.055980738282802846, -0.008415893602827644,...            1  \n",
       "..                                                 ...          ...  \n",
       "995  [[0.03410446276745788, -0.001262826572331346, ...            1  \n",
       "996  [[0.01446977716742693, -0.003531519313800009, ...            1  \n",
       "997  [[0.058890583197122814, 0.003327974210690777, ...            1  \n",
       "998  [[0.06409916711630012, 0.005045074107518778, -...            1  \n",
       "999  [[0.056282065514168526, -0.008710949602913673,...            1  \n",
       "\n",
       "[1000 rows x 114 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_sentences.preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [case, illegal, need, criticism, train]\n",
       "1                                     [several, armenia]\n",
       "2      [precondition, connect, soil, june, end, view,...\n",
       "3      [configure, democracy, exchequer, secure, dani...\n",
       "4      [therefore, refusal, council, absurd, situatio...\n",
       "                             ...                        \n",
       "995    [outcome, year, across, european, form, solida...\n",
       "996    [willing, afghanistan, man, woman, freedom, de...\n",
       "997    [member, therefore, document, answer, reply, r...\n",
       "998    [new, provision, product, almost, extend, vari...\n",
       "999    [obviously, exclude, satisfaction, union, addr...\n",
       "Name: translated_to_source_target, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_sentences.preprocessed.translated_to_source_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array_normalized_source, embedding_dictionary_source = load_embeddings(\n",
    "            embedding_array_source_path, embedding_dictionary_source_path)\n",
    "embedding_array_normalized_target, embedding_dictionary_target = load_embeddings(\n",
    "            embedding_array_target_path, embedding_dictionary_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vector_source = parallel_sentences.preprocessed.token_preprocessed_embedding_source\n",
    "token_vector_target = parallel_sentences.preprocessed.token_preprocessed_embedding_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_translation_dictionary(token_vector_source, token_vector_target, \n",
    "                                  embedding_array_normalized_source, embedding_dictionary_source, \n",
    "                                  embedding_array_normalized_target, embedding_dictionary_target):\n",
    "    unique_token_source = set([item for sublist in token_vector_source for item in sublist])\n",
    "    unique_token_target = set([item for sublist in token_vector_target for item in sublist])\n",
    "    \n",
    "    source_index = 0\n",
    "    word_embedding_dictionary_source = {}\n",
    "    embedding_subset_dictionary_source = {}\n",
    "    for token in unique_token_source:\n",
    "        if embedding_dictionary_source.get(token):\n",
    "                word_embedding_dictionary_source[token] = embedding_array_normalized_source[embedding_dictionary_source.get(token)].tolist()\n",
    "                embedding_subset_dictionary_source[source_index] = token\n",
    "                source_index += 1\n",
    "                \n",
    "    target_index = 0\n",
    "    word_embedding_dictionary_target = {}\n",
    "    embedding_subset_dictionary_target = {}\n",
    "    for token in unique_token_target:\n",
    "        if embedding_dictionary_target.get(token):\n",
    "                word_embedding_dictionary_target[token] = embedding_array_normalized_target[embedding_dictionary_target.get(token)].tolist()\n",
    "                embedding_subset_dictionary_target[target_index] = token\n",
    "                target_index += 1\n",
    "                \n",
    "    embedding_subset_source = np.array(list(word_embedding_dictionary_source.values()))\n",
    "    embedding_subset_target = np.array(list(word_embedding_dictionary_target.values()))\n",
    "    \n",
    "    def translation(token, word_embedding_dictionary_source, embedding_subset_target, embedding_subset_dictionary_target):\n",
    "        norm_src_word_emb = word_embedding_dictionary_source[token]\n",
    "        similarity_cos = np.dot(norm_src_word_emb, np.transpose(embedding_subset_target))\n",
    "        most_similar_trg_index = np.argsort(-similarity_cos)[0].tolist()\n",
    "        return embedding_subset_dictionary_target[most_similar_trg_index]\n",
    "    \n",
    "    translation_to_target_source = {}\n",
    "    for token in unique_token_source:\n",
    "        if embedding_dictionary_source.get(token):\n",
    "            translation_to_target_source[token] = translation(token, word_embedding_dictionary_source, embedding_subset_target, embedding_subset_dictionary_target)\n",
    "            \n",
    "    translation_to_source_target = {}\n",
    "    for token in unique_token_target:\n",
    "        if embedding_dictionary_target.get(token):\n",
    "            translation_to_source_target[token] = translation(token, word_embedding_dictionary_target, embedding_subset_source, embedding_subset_dictionary_source)\n",
    "            \n",
    "    return translation_to_target_source, translation_to_source_target\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_words(token_vector, translation_dictionary):\n",
    "    def calculate_translations(word_list, translation_dictionary):\n",
    "        translation_list = []\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                translation_list.append(translation_dictionary[word])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return translation_list\n",
    "\n",
    "    return token_vector.apply(lambda token_list: calculate_translations(token_list, translation_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_to_target_source, translation_to_source_target = create_translation_dictionary(token_vector_source, token_vector_target, \n",
    "                                  embedding_array_normalized_source, embedding_dictionary_source, \n",
    "                                  embedding_array_normalized_target, embedding_dictionary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sentences.preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_object(obj, filename):\n",
    "#     with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "#        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# sample usage\n",
    "# save_object(parallel_sentences, '../data/processed/processed_data_2505.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# filehandler = open('../data/processed/processed_data_2505_2.pkl', 'wb') \n",
    "# pickle.dump(parallel_sentences, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# file = open(\"../data/processed/processed_data_2505_2.pkl\",'rb')\n",
    "# df = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Create data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.data.dataset_class import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = DataSet(parallel_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = 90\n",
    "n_test_queries = 1\n",
    "n_test_documents = 10\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_sample(n_training, n_test_queries, n_test_documents,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II. Create sentence based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create features for our model, that are sentence based and should be created before the text is preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.features.feature_generation_class import FeatureGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "features_train = FeatureGeneration(dataset.dataset, number_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "features_train.feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.feature_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# filehandler = open('../data/processed/processed_data.pkl', 'wb') \n",
    "# pickle.dump(features.feature_dataframe, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# file = open(\"../data/processed/processed_data.pkl\",'rb')\n",
    "# df = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Unsupervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at correlation matrix\n",
    "np.cov(df, bias=True)\n",
    "corrMatrix=df.corr()\n",
    "f=plt.figure(figsize=(14,9))\n",
    "sn.heatmap(corrMatrix, annot=False)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korrelation\n",
    "correlated_features = set()\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            print(f\"The following features are correlated: {correlation_matrix.columns[i]} and {correlation_matrix.columns[j]}. Correlation = {round(abs(correlation_matrix.iloc[i, j]),2)}\")\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "print(f\"Drop the following features: {correlated_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop correlated features, but only when looking at a big dataset\n",
    "df=df.drop(columns=correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target=df['Translation']\n",
    "df=df.drop(columns=['Translation'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data otherwise logistic regression does not converge\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    df,target,test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "lr = LogisticRegression(class_weight = 'balanced', max_iter=10000).fit(data_train, target_train)\n",
    "prediction = lr.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "# get importance\n",
    "importance = lr.coef_[0]\n",
    "# summarize feature importance\n",
    "for i, v in enumerate(importance):\n",
    "    print(f'Feature: {i} {data_train.columns[i]}, Score: {v}')\n",
    "\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "nb = GaussianNB().fit(data_train, target_train)\n",
    "prediction = nb.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "svc = SVC().fit(data_train, target_train)\n",
    "prediction = svc.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "mlp = MLPClassifier().fit(data_train, target_train)\n",
    "prediction = mlp.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}