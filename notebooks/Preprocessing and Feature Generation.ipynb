{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing and Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook we import the data, preprocess the data and create features for supervised and unsupervised cross-lingual-information retrieval models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## I. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we import the English and German europarl datasets and combine them into a parallel sentence translation dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.import_data import create_data_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_data_subset(sentence_data_source='../data/external/europarl-v7.de-en.en',\n",
    "#                        sentences_data_target='../data/external/europarl-v7.de-en.de',\n",
    "#                        sample_size=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocessing_class import PreprocessingEuroParl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sentences = PreprocessingEuroParl(df_sampled_path=\"../data/interim/europarl_english_german.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parallel_sentences.dataframe = parallel_sentences.dataframe.iloc[0:5000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_source</th>\n",
       "      <th>text_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>If it is legal, we do not need a debate.</td>\n",
       "      <td>Falls es legal ist, dann brauchen wir keine De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14. Further macro-financial assistance for Geo...</td>\n",
       "      <td>14. Weitere Makrofinanzhilfe für Georgien (</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The request came to nothing, firstly because t...</td>\n",
       "      <td>Diese Forderung verlief im Sande, zum einen, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>That is how the judicial system works, and Lor...</td>\n",
       "      <td>So funktioniert der Rechtsstaat, und Lord Beth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Council's refusal to make the Charter of F...</td>\n",
       "      <td>Wir sind daher durch die Weigerung des Rates, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4995</td>\n",
       "      <td>May I end, Madam President, by expressing my s...</td>\n",
       "      <td>Gestatten Sie mir, Frau Präsidentin, abschließ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4996</td>\n",
       "      <td>That is my appeal - to reflect overnight and t...</td>\n",
       "      <td>Das ist mein Appell: Überdenken Sie das Ganze ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4997</td>\n",
       "      <td>Secondly, I speak as a European, to say that E...</td>\n",
       "      <td>In zweiter Linie spreche ich als Europäer und ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4998</td>\n",
       "      <td>The new approach being taken here should hopef...</td>\n",
       "      <td>Der neue Anlauf, den wir hiermit nehmen, soll ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4999</td>\n",
       "      <td>We have seen other outbreaks in other countrie...</td>\n",
       "      <td>Erst kürzlich haben wir von weiteren Fällen in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        text_source  \\\n",
       "0        0           If it is legal, we do not need a debate.   \n",
       "1        1  14. Further macro-financial assistance for Geo...   \n",
       "2        2  The request came to nothing, firstly because t...   \n",
       "3        3  That is how the judicial system works, and Lor...   \n",
       "4        4  The Council's refusal to make the Charter of F...   \n",
       "...    ...                                                ...   \n",
       "4995  4995  May I end, Madam President, by expressing my s...   \n",
       "4996  4996  That is my appeal - to reflect overnight and t...   \n",
       "4997  4997  Secondly, I speak as a European, to say that E...   \n",
       "4998  4998  The new approach being taken here should hopef...   \n",
       "4999  4999  We have seen other outbreaks in other countrie...   \n",
       "\n",
       "                                            text_target  \n",
       "0     Falls es legal ist, dann brauchen wir keine De...  \n",
       "1           14. Weitere Makrofinanzhilfe für Georgien (  \n",
       "2     Diese Forderung verlief im Sande, zum einen, w...  \n",
       "3     So funktioniert der Rechtsstaat, und Lord Beth...  \n",
       "4     Wir sind daher durch die Weigerung des Rates, ...  \n",
       "...                                                 ...  \n",
       "4995  Gestatten Sie mir, Frau Präsidentin, abschließ...  \n",
       "4996  Das ist mein Appell: Überdenken Sie das Ganze ...  \n",
       "4997  In zweiter Linie spreche ich als Europäer und ...  \n",
       "4998  Der neue Anlauf, den wir hiermit nehmen, soll ...  \n",
       "4999  Erst kürzlich haben wir von weiteren Fällen in...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_sentences.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #duc dataset\n",
    "# parallel_sentences.dataframe['Translation']=1\n",
    "# wrong= PreprocessingEuroParl(sentence_data_source='../data/external/europarl-v7.de-en.en',\n",
    "#                  sentence_data_target='../data/external/europarl-v7.de-en.de',number_datapoints=10000)\n",
    "# import pandas as pd\n",
    "# wrong_data=pd.concat([wrong.dataframe.drop(columns='text_target').reset_index(drop=True),wrong.dataframe['text_target'].sample(frac=1).reset_index(drop=True)],axis=1)\n",
    "# wrong_data['Translation']=0\n",
    "# data=pd.concat([parallel_sentences.dataframe.reset_index(drop=True),wrong_data.reset_index(drop=True)])\n",
    "# import pickle \n",
    "# filehandler = open('../data/processed/dataset_duc.pkl', 'wb') \n",
    "# pickle.dump(data, filehandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we preprocess the parallel sentence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob as textblob_source\n",
    "from textblob_de import TextBlobDE as textblob_target\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stopwords_source = stopwords.words('english')\n",
    "stopwords_target = stopwords.words('german')\n",
    "nlp_source = en_core_web_sm.load()\n",
    "nlp_target = de_core_news_sm.load()\n",
    "embedding_array_source_path = \"../data/interim/proc_5k_src_emb.pkl\"\n",
    "embedding_dictionary_source_path =  \"../data/interim/proc_5k_src_word.pkl\"\n",
    "embedding_array_target_path = \"../data/interim/proc_5k_trg_emb.pkl\"\n",
    "embedding_dictionary_target_path =  \"../data/interim/proc_5k_trg_word.pkl\"\n",
    "number_translations = 1\n",
    "number_pc = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "parallel_sentences.preprocess_sentences(stopwords_source, nlp_source, textblob_source,\n",
    "                                               embedding_array_source_path, embedding_dictionary_source_path,\n",
    "                                                stopwords_target,nlp_target, textblob_target,\n",
    "                                               embedding_array_target_path, embedding_dictionary_target_path,\n",
    "                                                number_translations, number_pc)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sentences.preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sentences.preprocessed.to_csv(\"safe.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_object(obj, filename):\n",
    "#     with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "#        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# sample usage\n",
    "# save_object(parallel_sentences, '../data/processed/processed_data_2505.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# filehandler = open('../data/processed/processed_data_2505_2.pkl', 'wb') \n",
    "# pickle.dump(parallel_sentences, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# file = open(\"../data/processed/processed_data_2505_2.pkl\",'rb')\n",
    "# df = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Create data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.data.dataset_class import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = DataSet(parallel_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = 90\n",
    "n_test_queries = 1\n",
    "n_test_documents = 10\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_sample(n_training, n_test_queries, n_test_documents,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_source</th>\n",
       "      <th>token_preprocessed_embedding_source</th>\n",
       "      <th>token_preprocessed_embedding_target</th>\n",
       "      <th>number_stopwords_source</th>\n",
       "      <th>number_stopwords_target</th>\n",
       "      <th>number_punctuations_total_source</th>\n",
       "      <th>number_punctuations_total_target</th>\n",
       "      <th>number_words_source</th>\n",
       "      <th>number_words_target</th>\n",
       "      <th>number_unique_words_source</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_embedding_average_source</th>\n",
       "      <th>sentence_embedding_average_target</th>\n",
       "      <th>sentence_embedding_tf_idf_source</th>\n",
       "      <th>sentence_embedding_tf_idf_target</th>\n",
       "      <th>pca_sentence_embedding_average_source</th>\n",
       "      <th>pca_sentence_embedding_average_target</th>\n",
       "      <th>pca_sentence_embedding_tf_idf_source</th>\n",
       "      <th>pca_sentence_embedding_tf_idf_target</th>\n",
       "      <th>Translation</th>\n",
       "      <th>id_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117</td>\n",
       "      <td>[disease, figure, show, clearly]</td>\n",
       "      <td>[krankheitsdaten, zeigen, deutlich]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.010017333086580038, 0.04874441109132022, -...</td>\n",
       "      <td>[[-0.004746901104226708, -0.06784963794052601,...</td>\n",
       "      <td>[[0.004067703506351096, 0.024935910994378894, ...</td>\n",
       "      <td>[[-0.0029277145638999247, -0.03933923996773458...</td>\n",
       "      <td>[[-0.12949643761385232, 0.1388400699943304, 0....</td>\n",
       "      <td>[[0.26272866129875183, -0.03103486355394125, -...</td>\n",
       "      <td>[[-0.06441677273812196, 0.06838918978053031, 0...</td>\n",
       "      <td>[[0.1496176470976494, -0.017388465353272466, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>[last, week, council, work, party, responsible...</td>\n",
       "      <td>[letzter, woche, zuständig, arbeitsgruppe, rat...</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.07326376087786186, 0.04503160322617207, -...</td>\n",
       "      <td>[[-0.07079099017816286, 0.03427593677285282, -...</td>\n",
       "      <td>[[-0.016079724419953166, 0.009913920003582708,...</td>\n",
       "      <td>[[-0.01883112217359222, 0.008916389397625055, ...</td>\n",
       "      <td>[[-0.06141113793654811, 0.09360336490152847, 0...</td>\n",
       "      <td>[[0.13937350587608913, 0.000491696848863891, 0...</td>\n",
       "      <td>[[-0.01507173788268915, 0.02049019557942409, 0...</td>\n",
       "      <td>[[0.03820079045448178, -0.0008272818679564057,...</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "      <td>[madam, president, welcome, mr, langen, 's, re...</td>\n",
       "      <td>[frau, präsidentin, begrüßen, bericht, herrn, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.03869397675485483, 0.03579077340795526, -...</td>\n",
       "      <td>[[-0.05850736474773536, 0.03301882650703192, -...</td>\n",
       "      <td>[[-0.009665046811339483, 0.008924232385536473,...</td>\n",
       "      <td>[[-0.017341574364950343, 0.008830848670690616,...</td>\n",
       "      <td>[[-0.07742600954536881, 0.052053494195986004, ...</td>\n",
       "      <td>[[0.08310952534278233, 0.03424447542056441, 0....</td>\n",
       "      <td>[[-0.02239219137628126, 0.012529661851771649, ...</td>\n",
       "      <td>[[0.025677220316657062, 0.010224607017932963, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129</td>\n",
       "      <td>[therefore, fully, support, approach, report, ...</td>\n",
       "      <td>[sinn, unterstützen, generell, ansatz, bericht...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.029104450872788828, 0.0033055458140249052...</td>\n",
       "      <td>[[-0.055084214028384954, 0.0014666318893432617...</td>\n",
       "      <td>[[-0.008622773546289321, 0.0009145870283218041...</td>\n",
       "      <td>[[-0.016315173101892493, -1.695980891757127e-0...</td>\n",
       "      <td>[[-0.14916455737936, 0.06393573126600434, 0.02...</td>\n",
       "      <td>[[0.22662464601712096, -0.04656096011038042, 0...</td>\n",
       "      <td>[[-0.04286179085424419, 0.019926089500480325, ...</td>\n",
       "      <td>[[0.06923584030865106, -0.01478614797449512, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95</td>\n",
       "      <td>[welcome, strengthening, trade, relation, prov...</td>\n",
       "      <td>[ausbau, handelsbeziehungen, begrüßen, sofern,...</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.05008048461362099, -0.045622791008402906,...</td>\n",
       "      <td>[[-0.03224296417708198, -0.03528159980972608, ...</td>\n",
       "      <td>[[-0.014451369339695508, -0.013126734759606613...</td>\n",
       "      <td>[[-0.010332043053378426, -0.011383004184929412...</td>\n",
       "      <td>[[-0.1551031737277905, 0.06398595481490095, 0....</td>\n",
       "      <td>[[0.24881377898984486, 0.022551155933696363, 0...</td>\n",
       "      <td>[[-0.04456170613949181, 0.01879428920192543, 0...</td>\n",
       "      <td>[[0.07767272363372349, 0.008039208965487389, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>38</td>\n",
       "      <td>[prove, impossible, get, council, commission, ...</td>\n",
       "      <td>[ergebnis, vorliegend, bericht, rücktritt, füh...</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.043347116979851344, 0.014273610826500732,...</td>\n",
       "      <td>[[-0.03156926203519106, 0.011037592416290532, ...</td>\n",
       "      <td>[[-0.009548701972919585, 0.002391840311229906,...</td>\n",
       "      <td>[[-0.008647833917607725, 0.0026701877388164634...</td>\n",
       "      <td>[[-0.15462185416127677, 0.07005518468339807, 0...</td>\n",
       "      <td>[[0.1398266918787902, 0.011092400855638763, 0....</td>\n",
       "      <td>[[-0.0317242380272315, 0.014733091576071637, -...</td>\n",
       "      <td>[[0.040598269927101915, 0.0029293871129557835,...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>190</td>\n",
       "      <td>[see, love, one, kill, war, yet, unable, reuni...</td>\n",
       "      <td>[erreichen, müssen, mitgliedstaaten, zusammena...</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.0388611298520118, 0.02212741502250234, -0...</td>\n",
       "      <td>[[-0.052134337835013866, 0.01858279202133417, ...</td>\n",
       "      <td>[[-0.010399794452965848, 0.005030973529368764,...</td>\n",
       "      <td>[[-0.02966943357626567, 0.008171631325840676, ...</td>\n",
       "      <td>[[-0.029443515402575334, 0.0987393864740928, 0...</td>\n",
       "      <td>[[0.22432024404406548, -0.05152263538911939, -...</td>\n",
       "      <td>[[-0.006774260985673793, 0.026174854559322492,...</td>\n",
       "      <td>[[0.10443982004797905, -0.027859116010950695, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>158</td>\n",
       "      <td>[exploit, much, ukrainian, people, profit]</td>\n",
       "      <td>[aussprache, schließen]</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.08924220204353332, 0.03766511082649231, -...</td>\n",
       "      <td>[[-0.06861083768308163, 0.006088372319936752, ...</td>\n",
       "      <td>[[-0.04001307538063078, 0.017034866933223033, ...</td>\n",
       "      <td>[[-0.04905906628671981, 0.0022598898558911645,...</td>\n",
       "      <td>[[-0.11853625131770969, 0.04598630052059889, 0...</td>\n",
       "      <td>[[0.15244074910879135, 0.009229101240634918, 0...</td>\n",
       "      <td>[[-0.05084606099253095, 0.014791298133105018, ...</td>\n",
       "      <td>[[0.10918291983873842, 0.008282805324573143, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>175</td>\n",
       "      <td>[midterm, review, envisage, case, believe, rig...</td>\n",
       "      <td>[lieben, kollegin, kollege, estland, lettland,...</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.052550282853189856, 0.03253021911950782, ...</td>\n",
       "      <td>[[-0.06996113783679903, 0.04964602146355901, -...</td>\n",
       "      <td>[[-0.013521540375747054, 0.007978567522369412,...</td>\n",
       "      <td>[[-0.023408381358570637, 0.016555929249164755,...</td>\n",
       "      <td>[[-0.1434008825744968, 0.09875457649468444, 0....</td>\n",
       "      <td>[[0.037553663831204176, -0.010458275210112333,...</td>\n",
       "      <td>[[-0.03674212653322596, 0.024424341110714978, ...</td>\n",
       "      <td>[[0.011415838371322113, -0.0021621320821200917...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>110</td>\n",
       "      <td>[sv, difficult, issue, present, modern, techno...</td>\n",
       "      <td>[mitgliedstaat, nämlich, droge, einseitig, leg...</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-0.03884335793554783, 0.005955601768458591, ...</td>\n",
       "      <td>[[-0.03730236366391182, 0.026905920278901856, ...</td>\n",
       "      <td>[[-0.009268365306488632, 0.0018037510260251837...</td>\n",
       "      <td>[[-0.012222781424431917, 0.009525867237180631,...</td>\n",
       "      <td>[[-0.14768797515288873, 0.059697304263382274, ...</td>\n",
       "      <td>[[0.17828956618905067, -0.03835043590515852, 0...</td>\n",
       "      <td>[[-0.03494389937126422, 0.013646707137759492, ...</td>\n",
       "      <td>[[0.06426098016749661, -0.013900119707019404, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_source                token_preprocessed_embedding_source  \\\n",
       "0          117                   [disease, figure, show, clearly]   \n",
       "1           85  [last, week, council, work, party, responsible...   \n",
       "2          149  [madam, president, welcome, mr, langen, 's, re...   \n",
       "3          129  [therefore, fully, support, approach, report, ...   \n",
       "4           95  [welcome, strengthening, trade, relation, prov...   \n",
       "..         ...                                                ...   \n",
       "175         38  [prove, impossible, get, council, commission, ...   \n",
       "176        190  [see, love, one, kill, war, yet, unable, reuni...   \n",
       "177        158         [exploit, much, ukrainian, people, profit]   \n",
       "178        175  [midterm, review, envisage, case, believe, rig...   \n",
       "179        110  [sv, difficult, issue, present, modern, techno...   \n",
       "\n",
       "                   token_preprocessed_embedding_target  \\\n",
       "0                  [krankheitsdaten, zeigen, deutlich]   \n",
       "1    [letzter, woche, zuständig, arbeitsgruppe, rat...   \n",
       "2    [frau, präsidentin, begrüßen, bericht, herrn, ...   \n",
       "3    [sinn, unterstützen, generell, ansatz, bericht...   \n",
       "4    [ausbau, handelsbeziehungen, begrüßen, sofern,...   \n",
       "..                                                 ...   \n",
       "175  [ergebnis, vorliegend, bericht, rücktritt, füh...   \n",
       "176  [erreichen, müssen, mitgliedstaaten, zusammena...   \n",
       "177                            [aussprache, schließen]   \n",
       "178  [lieben, kollegin, kollege, estland, lettland,...   \n",
       "179  [mitgliedstaat, nämlich, droge, einseitig, leg...   \n",
       "\n",
       "     number_stopwords_source  number_stopwords_target  \\\n",
       "0                          3                        3   \n",
       "1                         14                       16   \n",
       "2                          5                        8   \n",
       "3                          9                       11   \n",
       "4                         14                       11   \n",
       "..                       ...                      ...   \n",
       "175                       20                       13   \n",
       "176                       17                        6   \n",
       "177                        9                        3   \n",
       "178                       21                        5   \n",
       "179                       12                        7   \n",
       "\n",
       "     number_punctuations_total_source  number_punctuations_total_target  \\\n",
       "0                                   0                                 0   \n",
       "1                                   2                                 3   \n",
       "2                                   1                                 1   \n",
       "3                                   2                                 1   \n",
       "4                                   1                                 2   \n",
       "..                                ...                               ...   \n",
       "175                                 4                                 2   \n",
       "176                                 0                                 0   \n",
       "177                                 2                                 0   \n",
       "178                                 1                                 1   \n",
       "179                                 3                                 1   \n",
       "\n",
       "     number_words_source  number_words_target  number_unique_words_source  \\\n",
       "0                      4                    3                           4   \n",
       "1                     21                   15                          21   \n",
       "2                     15                   13                          15   \n",
       "3                     12                   11                          12   \n",
       "4                     12                   10                          12   \n",
       "..                   ...                  ...                         ...   \n",
       "175                   27                   12                          23   \n",
       "176                   16                    4                          16   \n",
       "177                    5                    2                           5   \n",
       "178                   16                    9                          16   \n",
       "179                   17                    8                          17   \n",
       "\n",
       "     ...                  sentence_embedding_average_source  \\\n",
       "0    ...  [[0.010017333086580038, 0.04874441109132022, -...   \n",
       "1    ...  [[-0.07326376087786186, 0.04503160322617207, -...   \n",
       "2    ...  [[-0.03869397675485483, 0.03579077340795526, -...   \n",
       "3    ...  [[-0.029104450872788828, 0.0033055458140249052...   \n",
       "4    ...  [[-0.05008048461362099, -0.045622791008402906,...   \n",
       "..   ...                                                ...   \n",
       "175  ...  [[-0.043347116979851344, 0.014273610826500732,...   \n",
       "176  ...  [[-0.0388611298520118, 0.02212741502250234, -0...   \n",
       "177  ...  [[-0.08924220204353332, 0.03766511082649231, -...   \n",
       "178  ...  [[-0.052550282853189856, 0.03253021911950782, ...   \n",
       "179  ...  [[-0.03884335793554783, 0.005955601768458591, ...   \n",
       "\n",
       "                     sentence_embedding_average_target  \\\n",
       "0    [[-0.004746901104226708, -0.06784963794052601,...   \n",
       "1    [[-0.07079099017816286, 0.03427593677285282, -...   \n",
       "2    [[-0.05850736474773536, 0.03301882650703192, -...   \n",
       "3    [[-0.055084214028384954, 0.0014666318893432617...   \n",
       "4    [[-0.03224296417708198, -0.03528159980972608, ...   \n",
       "..                                                 ...   \n",
       "175  [[-0.03156926203519106, 0.011037592416290532, ...   \n",
       "176  [[-0.052134337835013866, 0.01858279202133417, ...   \n",
       "177  [[-0.06861083768308163, 0.006088372319936752, ...   \n",
       "178  [[-0.06996113783679903, 0.04964602146355901, -...   \n",
       "179  [[-0.03730236366391182, 0.026905920278901856, ...   \n",
       "\n",
       "                      sentence_embedding_tf_idf_source  \\\n",
       "0    [[0.004067703506351096, 0.024935910994378894, ...   \n",
       "1    [[-0.016079724419953166, 0.009913920003582708,...   \n",
       "2    [[-0.009665046811339483, 0.008924232385536473,...   \n",
       "3    [[-0.008622773546289321, 0.0009145870283218041...   \n",
       "4    [[-0.014451369339695508, -0.013126734759606613...   \n",
       "..                                                 ...   \n",
       "175  [[-0.009548701972919585, 0.002391840311229906,...   \n",
       "176  [[-0.010399794452965848, 0.005030973529368764,...   \n",
       "177  [[-0.04001307538063078, 0.017034866933223033, ...   \n",
       "178  [[-0.013521540375747054, 0.007978567522369412,...   \n",
       "179  [[-0.009268365306488632, 0.0018037510260251837...   \n",
       "\n",
       "                      sentence_embedding_tf_idf_target  \\\n",
       "0    [[-0.0029277145638999247, -0.03933923996773458...   \n",
       "1    [[-0.01883112217359222, 0.008916389397625055, ...   \n",
       "2    [[-0.017341574364950343, 0.008830848670690616,...   \n",
       "3    [[-0.016315173101892493, -1.695980891757127e-0...   \n",
       "4    [[-0.010332043053378426, -0.011383004184929412...   \n",
       "..                                                 ...   \n",
       "175  [[-0.008647833917607725, 0.0026701877388164634...   \n",
       "176  [[-0.02966943357626567, 0.008171631325840676, ...   \n",
       "177  [[-0.04905906628671981, 0.0022598898558911645,...   \n",
       "178  [[-0.023408381358570637, 0.016555929249164755,...   \n",
       "179  [[-0.012222781424431917, 0.009525867237180631,...   \n",
       "\n",
       "                 pca_sentence_embedding_average_source  \\\n",
       "0    [[-0.12949643761385232, 0.1388400699943304, 0....   \n",
       "1    [[-0.06141113793654811, 0.09360336490152847, 0...   \n",
       "2    [[-0.07742600954536881, 0.052053494195986004, ...   \n",
       "3    [[-0.14916455737936, 0.06393573126600434, 0.02...   \n",
       "4    [[-0.1551031737277905, 0.06398595481490095, 0....   \n",
       "..                                                 ...   \n",
       "175  [[-0.15462185416127677, 0.07005518468339807, 0...   \n",
       "176  [[-0.029443515402575334, 0.0987393864740928, 0...   \n",
       "177  [[-0.11853625131770969, 0.04598630052059889, 0...   \n",
       "178  [[-0.1434008825744968, 0.09875457649468444, 0....   \n",
       "179  [[-0.14768797515288873, 0.059697304263382274, ...   \n",
       "\n",
       "                 pca_sentence_embedding_average_target  \\\n",
       "0    [[0.26272866129875183, -0.03103486355394125, -...   \n",
       "1    [[0.13937350587608913, 0.000491696848863891, 0...   \n",
       "2    [[0.08310952534278233, 0.03424447542056441, 0....   \n",
       "3    [[0.22662464601712096, -0.04656096011038042, 0...   \n",
       "4    [[0.24881377898984486, 0.022551155933696363, 0...   \n",
       "..                                                 ...   \n",
       "175  [[0.1398266918787902, 0.011092400855638763, 0....   \n",
       "176  [[0.22432024404406548, -0.05152263538911939, -...   \n",
       "177  [[0.15244074910879135, 0.009229101240634918, 0...   \n",
       "178  [[0.037553663831204176, -0.010458275210112333,...   \n",
       "179  [[0.17828956618905067, -0.03835043590515852, 0...   \n",
       "\n",
       "                  pca_sentence_embedding_tf_idf_source  \\\n",
       "0    [[-0.06441677273812196, 0.06838918978053031, 0...   \n",
       "1    [[-0.01507173788268915, 0.02049019557942409, 0...   \n",
       "2    [[-0.02239219137628126, 0.012529661851771649, ...   \n",
       "3    [[-0.04286179085424419, 0.019926089500480325, ...   \n",
       "4    [[-0.04456170613949181, 0.01879428920192543, 0...   \n",
       "..                                                 ...   \n",
       "175  [[-0.0317242380272315, 0.014733091576071637, -...   \n",
       "176  [[-0.006774260985673793, 0.026174854559322492,...   \n",
       "177  [[-0.05084606099253095, 0.014791298133105018, ...   \n",
       "178  [[-0.03674212653322596, 0.024424341110714978, ...   \n",
       "179  [[-0.03494389937126422, 0.013646707137759492, ...   \n",
       "\n",
       "                  pca_sentence_embedding_tf_idf_target  Translation  id_target  \n",
       "0    [[0.1496176470976494, -0.017388465353272466, -...            1        117  \n",
       "1    [[0.03820079045448178, -0.0008272818679564057,...            1         85  \n",
       "2    [[0.025677220316657062, 0.010224607017932963, ...            1        149  \n",
       "3    [[0.06923584030865106, -0.01478614797449512, 0...            1        129  \n",
       "4    [[0.07767272363372349, 0.008039208965487389, 0...            1         95  \n",
       "..                                                 ...          ...        ...  \n",
       "175  [[0.040598269927101915, 0.0029293871129557835,...            0         45  \n",
       "176  [[0.10443982004797905, -0.027859116010950695, ...            0         68  \n",
       "177  [[0.10918291983873842, 0.008282805324573143, 0...            0         88  \n",
       "178  [[0.011415838371322113, -0.0021621320821200917...            0         63  \n",
       "179  [[0.06426098016749661, -0.013900119707019404, ...            0        102  \n",
       "\n",
       "[180 rows x 143 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II. Create sentence based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create features for our model, that are sentence based and should be created before the text is preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.features.feature_generation_class import FeatureGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "features_train = FeatureGeneration(dataset.dataset, number_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakob/PycharmProjects/crosslingual-information-retrieval/src/features/sentence_based.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  return (target_array - source_array).replace(np.nan, 0).replace(np.inf, 0).replace(np.log(0), 0)\n",
      "/Users/jakob/PycharmProjects/crosslingual-information-retrieval/src/features/sentence_based.py:45: RuntimeWarning: divide by zero encountered in log\n",
      "  return ((target_array - source_array) / source_array).replace(np.nan, 0).replace(np.inf, 0).replace(np.log(0), 0)\n",
      "/Users/jakob/PycharmProjects/crosslingual-information-retrieval/src/features/sentence_based.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "  return ((source_array / source_sentence_length) - (target_array / target_sentence_length)).replace(np.nan, 0).replace(np.inf, 0).replace(np.log(0), 0)\n"
     ]
    }
   ],
   "source": [
    "features_train.feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_punctuations_total_difference</th>\n",
       "      <th>number_punctuations_total_difference_relative</th>\n",
       "      <th>number_punctuations_total_difference_normalized</th>\n",
       "      <th>number_words_difference</th>\n",
       "      <th>number_words_difference_relative</th>\n",
       "      <th>number_words_difference_normalized</th>\n",
       "      <th>number_unique_words_difference</th>\n",
       "      <th>number_unique_words_difference_relative</th>\n",
       "      <th>number_unique_words_difference_normalized</th>\n",
       "      <th>number_!_difference</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_embeddding_tf_idf_diff_5</th>\n",
       "      <th>pca_embeddding_average_diff_6</th>\n",
       "      <th>pca_embeddding_tf_idf_diff_6</th>\n",
       "      <th>pca_embeddding_average_diff_7</th>\n",
       "      <th>pca_embeddding_tf_idf_diff_7</th>\n",
       "      <th>pca_embeddding_average_diff_8</th>\n",
       "      <th>pca_embeddding_tf_idf_diff_8</th>\n",
       "      <th>pca_embeddding_average_diff_9</th>\n",
       "      <th>pca_embeddding_tf_idf_diff_9</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053989</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>-0.000616</td>\n",
       "      <td>0.101184</td>\n",
       "      <td>0.057296</td>\n",
       "      <td>0.094184</td>\n",
       "      <td>0.054041</td>\n",
       "      <td>-0.013247</td>\n",
       "      <td>-0.005876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.079710</td>\n",
       "      <td>-6</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>-6</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>-0.032987</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>-0.019100</td>\n",
       "      <td>-0.001928</td>\n",
       "      <td>0.054891</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>-0.009564</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>-0.064867</td>\n",
       "      <td>-0.017260</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>-0.046337</td>\n",
       "      <td>-0.016028</td>\n",
       "      <td>-0.019219</td>\n",
       "      <td>-0.004022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.059524</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.059524</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043491</td>\n",
       "      <td>-0.041101</td>\n",
       "      <td>-0.009994</td>\n",
       "      <td>0.027708</td>\n",
       "      <td>0.008766</td>\n",
       "      <td>0.057870</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>-0.042500</td>\n",
       "      <td>-0.013060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.089744</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>-0.082700</td>\n",
       "      <td>-0.024513</td>\n",
       "      <td>0.033305</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.039707</td>\n",
       "      <td>-0.076313</td>\n",
       "      <td>-0.022338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-2</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.013825</td>\n",
       "      <td>-15</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>-11</td>\n",
       "      <td>-0.478261</td>\n",
       "      <td>-0.115207</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017916</td>\n",
       "      <td>-0.019759</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>0.027083</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.015695</td>\n",
       "      <td>0.008348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017273</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>0.011901</td>\n",
       "      <td>0.066914</td>\n",
       "      <td>0.022881</td>\n",
       "      <td>-0.007793</td>\n",
       "      <td>-0.007881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033186</td>\n",
       "      <td>-0.024890</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.119780</td>\n",
       "      <td>-0.061739</td>\n",
       "      <td>0.059441</td>\n",
       "      <td>0.044727</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.005996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.041176</td>\n",
       "      <td>-7</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>0.041176</td>\n",
       "      <td>-7</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>0.041176</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017414</td>\n",
       "      <td>-0.058098</td>\n",
       "      <td>-0.014064</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>-0.028255</td>\n",
       "      <td>-0.007734</td>\n",
       "      <td>-0.037849</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>-2</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>-9</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-9</td>\n",
       "      <td>-0.529412</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007607</td>\n",
       "      <td>-0.059052</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>0.023302</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>-0.008799</td>\n",
       "      <td>-0.004187</td>\n",
       "      <td>-0.007236</td>\n",
       "      <td>-0.000845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number_punctuations_total_difference  \\\n",
       "0                                       0   \n",
       "1                                       1   \n",
       "2                                       0   \n",
       "3                                      -1   \n",
       "4                                       1   \n",
       "..                                    ...   \n",
       "175                                    -2   \n",
       "176                                     0   \n",
       "177                                    -2   \n",
       "178                                     0   \n",
       "179                                    -2   \n",
       "\n",
       "     number_punctuations_total_difference_relative  \\\n",
       "0                                         0.000000   \n",
       "1                                         0.500000   \n",
       "2                                         0.000000   \n",
       "3                                        -0.500000   \n",
       "4                                         1.000000   \n",
       "..                                             ...   \n",
       "175                                      -0.500000   \n",
       "176                                       0.000000   \n",
       "177                                      -1.000000   \n",
       "178                                       0.000000   \n",
       "179                                      -0.666667   \n",
       "\n",
       "     number_punctuations_total_difference_normalized  number_words_difference  \\\n",
       "0                                           0.000000                       -1   \n",
       "1                                          -0.079710                       -6   \n",
       "2                                          -0.008929                       -2   \n",
       "3                                           0.059524                       -1   \n",
       "4                                          -0.089744                       -2   \n",
       "..                                               ...                      ...   \n",
       "175                                        -0.013825                      -15   \n",
       "176                                         0.000000                      -12   \n",
       "177                                         0.285714                       -3   \n",
       "178                                        -0.041176                       -7   \n",
       "179                                         0.038889                       -9   \n",
       "\n",
       "     number_words_difference_relative  number_words_difference_normalized  \\\n",
       "0                           -0.250000                            0.000000   \n",
       "1                           -0.285714                            0.079710   \n",
       "2                           -0.133333                            0.008929   \n",
       "3                           -0.083333                           -0.059524   \n",
       "4                           -0.166667                            0.089744   \n",
       "..                                ...                                 ...   \n",
       "175                         -0.555556                            0.013825   \n",
       "176                         -0.750000                            0.000000   \n",
       "177                         -0.600000                           -0.285714   \n",
       "178                         -0.437500                            0.041176   \n",
       "179                         -0.529412                           -0.038889   \n",
       "\n",
       "     number_unique_words_difference  number_unique_words_difference_relative  \\\n",
       "0                                -1                                -0.250000   \n",
       "1                                -6                                -0.285714   \n",
       "2                                -2                                -0.133333   \n",
       "3                                -1                                -0.083333   \n",
       "4                                -2                                -0.166667   \n",
       "..                              ...                                      ...   \n",
       "175                             -11                                -0.478261   \n",
       "176                             -12                                -0.750000   \n",
       "177                              -3                                -0.600000   \n",
       "178                              -7                                -0.437500   \n",
       "179                              -9                                -0.529412   \n",
       "\n",
       "     number_unique_words_difference_normalized  number_!_difference  ...  \\\n",
       "0                                     0.000000                    0  ...   \n",
       "1                                     0.079710                    0  ...   \n",
       "2                                     0.008929                    0  ...   \n",
       "3                                    -0.059524                    0  ...   \n",
       "4                                     0.089744                    0  ...   \n",
       "..                                         ...                  ...  ...   \n",
       "175                                  -0.115207                    0  ...   \n",
       "176                                   0.000000                    0  ...   \n",
       "177                                  -0.285714                    0  ...   \n",
       "178                                   0.041176                    1  ...   \n",
       "179                                  -0.038889                    0  ...   \n",
       "\n",
       "     pca_embeddding_tf_idf_diff_5  pca_embeddding_average_diff_6  \\\n",
       "0                       -0.053989                       0.001882   \n",
       "1                        0.001370                      -0.032987   \n",
       "2                       -0.011399                      -0.064867   \n",
       "3                       -0.043491                      -0.041101   \n",
       "4                       -0.029210                      -0.082700   \n",
       "..                            ...                            ...   \n",
       "175                     -0.017916                      -0.019759   \n",
       "176                      0.017273                       0.010295   \n",
       "177                     -0.033186                      -0.024890   \n",
       "178                     -0.017414                      -0.058098   \n",
       "179                     -0.007607                      -0.059052   \n",
       "\n",
       "     pca_embeddding_tf_idf_diff_6  pca_embeddding_average_diff_7  \\\n",
       "0                       -0.000616                       0.101184   \n",
       "1                       -0.005607                      -0.019100   \n",
       "2                       -0.017260                       0.002975   \n",
       "3                       -0.009994                       0.027708   \n",
       "4                       -0.024513                       0.033305   \n",
       "..                            ...                            ...   \n",
       "175                     -0.000520                       0.027083   \n",
       "176                      0.000567                      -0.000344   \n",
       "177                     -0.005404                      -0.119780   \n",
       "178                     -0.014064                      -0.001037   \n",
       "179                     -0.010111                       0.023302   \n",
       "\n",
       "     pca_embeddding_tf_idf_diff_7  pca_embeddding_average_diff_8  \\\n",
       "0                        0.057296                       0.094184   \n",
       "1                       -0.001928                       0.054891   \n",
       "2                        0.004078                      -0.046337   \n",
       "3                        0.008766                       0.057870   \n",
       "4                        0.012570                       0.127930   \n",
       "..                            ...                            ...   \n",
       "175                      0.009870                       0.009747   \n",
       "176                      0.011901                       0.066914   \n",
       "177                     -0.061739                       0.059441   \n",
       "178                      0.003987                      -0.028255   \n",
       "179                      0.009772                      -0.008799   \n",
       "\n",
       "     pca_embeddding_tf_idf_diff_8  pca_embeddding_average_diff_9  \\\n",
       "0                        0.054041                      -0.013247   \n",
       "1                        0.013568                      -0.009564   \n",
       "2                       -0.016028                      -0.019219   \n",
       "3                        0.015263                      -0.042500   \n",
       "4                        0.039707                      -0.076313   \n",
       "..                            ...                            ...   \n",
       "175                      0.001344                       0.015695   \n",
       "176                      0.022881                      -0.007793   \n",
       "177                      0.044727                       0.006044   \n",
       "178                     -0.007734                      -0.037849   \n",
       "179                     -0.004187                      -0.007236   \n",
       "\n",
       "     pca_embeddding_tf_idf_diff_9  Translation  \n",
       "0                       -0.005876            1  \n",
       "1                       -0.000839            1  \n",
       "2                       -0.004022            1  \n",
       "3                       -0.013060            1  \n",
       "4                       -0.022338            1  \n",
       "..                            ...          ...  \n",
       "175                      0.008348            0  \n",
       "176                     -0.007881            0  \n",
       "177                      0.005996            0  \n",
       "178                     -0.012464            0  \n",
       "179                     -0.000845            0  \n",
       "\n",
       "[180 rows x 209 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.feature_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# filehandler = open('../data/processed/processed_data.pkl', 'wb') \n",
    "# pickle.dump(features.feature_dataframe, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# file = open(\"../data/processed/processed_data.pkl\",'rb')\n",
    "# df = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Unsupervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at correlation matrix\n",
    "np.cov(df, bias=True)\n",
    "corrMatrix=df.corr()\n",
    "f=plt.figure(figsize=(14,9))\n",
    "sn.heatmap(corrMatrix, annot=False)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korrelation\n",
    "correlated_features = set()\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            print(f\"The following features are correlated: {correlation_matrix.columns[i]} and {correlation_matrix.columns[j]}. Correlation = {round(abs(correlation_matrix.iloc[i, j]),2)}\")\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "print(f\"Drop the following features: {correlated_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop correlated features, but only when looking at a big dataset\n",
    "df=df.drop(columns=correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target=df['Translation']\n",
    "df=df.drop(columns=['Translation'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data otherwise logistic regression does not converge\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    df,target,test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "lr = LogisticRegression(class_weight = 'balanced', max_iter=10000).fit(data_train, target_train)\n",
    "prediction = lr.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "# get importance\n",
    "importance = lr.coef_[0]\n",
    "# summarize feature importance\n",
    "for i, v in enumerate(importance):\n",
    "    print(f'Feature: {i} {data_train.columns[i]}, Score: {v}')\n",
    "\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "nb = GaussianNB().fit(data_train, target_train)\n",
    "prediction = nb.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "svc = SVC().fit(data_train, target_train)\n",
    "prediction = svc.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "mlp = MLPClassifier().fit(data_train, target_train)\n",
    "prediction = mlp.predict(data_test)\n",
    "acc = accuracy_score(target_test,prediction) \n",
    "f1= f1_score(target_test,prediction) \n",
    "pr= precision_score(target_test,prediction) \n",
    "re= recall_score(target_test,prediction) \n",
    "print(\"The Accuracy on test set: {:.4f}\".format(acc))\n",
    "print(\"The F1-Score on test set: {:.4f}\".format(f1))\n",
    "print(\"The Precision-Score on test set: {:.4f}\".format(pr))\n",
    "print(\"The Recall-Score on test set: {:.4f}\".format(re))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
