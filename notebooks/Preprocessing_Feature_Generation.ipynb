{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing and Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook we import the data, preprocess the data and create features for supervised and unsupervised cross-lingual-information retrieval models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## I. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we import the English and German europarl datasets and combine them into a parallel sentence translation dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import import_data\n",
    "importlib.reload(import_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data.create_data_subset(sentence_data_source='../data/external/europarl-v7.de-en.en',\n",
    "                      sentences_data_target='../data/external/europarl-v7.de-en.de',\n",
    "                      sample_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## II. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we preprocess the parallel sentence data for the feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob as textblob_source\n",
    "from textblob_de import TextBlobDE as textblob_target\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import preprocessing_class\n",
    "importlib.reload(preprocessing_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stopwords_source = stopwords.words('english')\n",
    "stopwords_target = stopwords.words('german')\n",
    "nlp_source = en_core_web_sm.load()\n",
    "nlp_target = de_core_news_sm.load()\n",
    "embedding_array_source_path = \"../data/interim/en_de_proc_5k_src_emb.pkl\"\n",
    "embedding_dictionary_source_path =  \"../data/interim/en_de_proc_5k_src_word.pkl\"\n",
    "embedding_array_target_path = \"../data/interim/en_de_proc_5k_trg_emb.pkl\"\n",
    "embedding_dictionary_target_path =  \"../data/interim/en_de_proc_5k_trg_word.pkl\"\n",
    "number_pc = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sentences = preprocessing_class.PreprocessingEuroParl(df_sampled_path=\n",
    "                                                               \"../data/interim/europarl_english_german.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "parallel_sentences.preprocess_sentences(stopwords_source, nlp_source, textblob_source,\n",
    "                                                embedding_array_source_path, embedding_dictionary_source_path,\n",
    "                                                stopwords_target,nlp_target, textblob_target,\n",
    "                                                embedding_array_target_path, embedding_dictionary_target_path,\n",
    "                                                number_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_sentences.preprocessed.to_json(\"../data/interim/preprocessed_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Create data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create the datasets for the training of the supervised model and the data for the supervised and unsupervised retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# preprocessed_data = pd.read_json(\"../data/interim/preprocessed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.data import dataset_class\n",
    "importlib.reload(dataset_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model = 20000\n",
    "n_queries = 100\n",
    "n_retrieval = 5000\n",
    "k = 5\n",
    "sample_size_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset_class.DataSet(parallel_sentences.preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split_model_retrieval(n_model, n_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.generate_model_dataset(n_model, k, sample_size_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.generate_retrieval_dataset(n_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IV. Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we create features for our model, that are sentence based and should be created before the text is preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.features import feature_generation_class\n",
    "importlib.reload(feature_generation_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_pc = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of the training data for the supervised classifciation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "features_model = feature_generation_class.FeatureGeneration(dataset.model_dataset, number_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "features_model.feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_model.feature_dataframe.reset_index(drop=True).to_json(\"../data/processed/feature_dataframe.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of the data for the crosslingual information retrieval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_retrieval = feature_generation_class.FeatureGeneration(dataset.retrieval_dataset, number_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_retrieval.feature_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_retrieval.feature_dataframe.reset_index(drop=True).to_json(\"../data/processed/feature_retrieval.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}